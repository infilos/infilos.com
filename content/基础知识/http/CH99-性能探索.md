---
type: docs
title: "性能探索"
linkTitle: "性能探索"
weight: 99
---

[Ideal HTTP Performance](https://www.mnot.net/blog/2016/04/22/ideal-http)一文的翻译，点击查看原文。

Web性能的隐性规则是减少终端用户能够感知到的延迟；在用户之前获得页面并使交互变得尽可能的快。

至于HTTP而言，这意味着一个理想的协议交互看起来像这样：

<div align="center"> <img src="https://infi-img.oss-cn-hangzhou.aliyuncs.com/img/WEB%E6%80%A7%E8%83%BD%E7%90%86%E6%83%B3%E7%9A%84%E5%8D%8F%E8%AE%AE%E4%BA%A4%E4%BA%92.jpg" style="display:block;width:50%;" alt="NAME" align=center /> </div>

意思就是一个页面的加载过程中，需要**在最少的往返次数中，发送尽可能少的数据到服务端，然后下载尽可能少的必须的数据**。

额外数数据同时意味着更多的转换时间和更多的出错机会，比如拥堵或者丢包，这将严重的影响性能。

由于协议“隔阂(chattiness)”的更多往返次数会带来更多的延迟，尤其是移动网络(一个往返100ms可以作为你最好的预期)。

如果这是最理想的情况，那HTTP是如何度量的？我们又如何来提升呢？

## HTTP/1.1

多种原因说明HTTP/1.1是一个很好的协议，但遗憾的是现代的Web工作方式意味着性能并不是其中之一。一个典型的页面加载方式看起来会是这样：

<div align="center"> <img src="https://infi-img.oss-cn-hangzhou.aliyuncs.com/img/HTTP1.1.jpg" style="display:block;width:50%;" alt="NAME" align=center /> </div>

这并不是特别的理想。

Web使用的HTTP/1是非常“饶舌”的，因为客户端需要多次去请求服务端以寻求它需要的更多东西；首先是HTML，然后是CSS和Javascript。每次这样的交互都会增加新的一个或多个往返从而增加了页面加载的延迟，这与理想中的“最少的往返次数”相悖。

此外，仅对页面的请求就增加了大量的数据，这与理想中的“发送尽可能少的数据到服务端”相悖。这是因为比如`Referer、User-Agent、Cookie`这样的冗长Header信息会在每次请求中重复，并且又被大量的Web页面所需要的资源加倍增长。

最终，由于HTTP/1的`head-of-line blocking`问题(线头阻塞:队列首个packet由于他的目的端口正忙而被延迟转发)，将多个资源组装到一个大的CSS代码、内嵌或连接，成为了一个普遍的最佳实践。这些都是HTTP/1中漂亮的性能hack，但是他们同样有一个损失：他们下载了远多于客户端需求的数据来显示一个页面，这与我们的理想相悖，并不能做到尽可能快的展示页面。

总的来说，HTTP/1并是不是一无是处，性能上的智能。举例来说，它拥有缓存，允许你在一个新的拷贝时完全不需要网络。还有受限制的请求，当有一个老的拷贝时避免你去转换大的东西。

## HTTP/2

HTTP/2试图通过几种方式去解决HTTP/1.1中的问题：

1. 完全的**复用**意味着线头阻塞不再是问题。可以通过单个HTTP连接加载整个Web页面，而不用担心创建了多少个请求。数据浪费的优化技巧可以被丢下了。
2. Header压缩移除了由于冗长头部信息引起的单个消息的消耗。现在可以将数十个甚至数百个请求合并到仅仅一些IP包中。这更接近于理想中的两种“更少的数据”。
3. HTTP/2的**服务端PUSH**主动提供客户端的需要，避免更多的往复次数带来的消耗。

因此，HTTP/2的运作看起来会像这样：

<div align="center"> <img src="https://infi-img.oss-cn-hangzhou.aliyuncs.com/img/HTTP2.jpg" style="display:block;width:50%;" alt="NAME" align=center /> </div>

图中可以看到服务端在没有请求的情况下向客户端发送CSS、Javascript和图片。它知道客户端可能将会请求这些，因此它使用**Server Push**发送合成的请求/响应对到客户端，以节省一个往复次数。它不再是一个“饶舌”的协议，结果是他能更充分的使用网络。

需要注意的是，并不是说这些会更简单。HTTP/2仍然有很多存在的问题，特别是关于推送的时机。

## HTTP/2 + 缓存摘要

服务端PUSH一个通常的问题是“客户端的缓存中是否已经有一个拷贝？”，因为推送天生是投机性的，总会出现你推送的东西并不是客户端需要的。

HTTP/2允许客户端在这种情况下通过`RESET_STREAM`取消推送。然而尽管这样，仍然会有一次往复的交互被浪费了，或许他们可以用来处理更值得的事情。记住，理想的是仅发送客户端用于显示页面仅需要的数据。

一个提议的方案是客户端使用一个紧凑的`Cache Digest`来告诉服务端它已经拥有的cache，从而使得服务端知道哪些是需要的。

<div align="center"> <img src="https://infi-img.oss-cn-hangzhou.aliyuncs.com/img/HTTP2_cache.jpg" style="display:block;width:50%;" alt="NAME" align=center /> </div>

因为缓存摘要使用`Golumb Compressed Sets`，它实时计算并使用少一1000个字节将浏览器缓存告诉服务端，通过连接在前几个包发送给服务端。

现在，我们避免了额外的往复次数，相关的数据浪费，直接嵌入和一些相似的hack，和非必须请求的数据浪费。这使我们里理想更进一步！

缓存摘要只是一个提议，但在HTTP社区中已经表现出很明显的兴趣。

## TCP

目前为止，并没有谈到浏览器加载Web页面所使用的其他协议对性能的影响。

然而，真正的问题要比下面图片暗示的要多，TCP在HTTP开始前需要三次握手，来协定一个新连接的参数。

<div align="center"> <img src="https://infi-img.oss-cn-hangzhou.aliyuncs.com/img/TCP.jpg" style="display:block;width:50%;" alt="NAME" align=center /> </div>

这意味着一个连接的创建所需要的最少往返次数，这使每个新连接的创建增加了额外的。

`TCP Fast Open`技术(在TCP3次握手的同时也进行数据交换)允许应用在`SYN`和`SYN+ACK`包中发送数据以避免这样的消耗。遗憾的是，仅被Linux和OSX支持。并且，社区的开发刚刚起步，在HTTP中使用TFO仍然有很多棘手的问题。

换句话说，TFO并不能保证随着SYN包发出的数据只会出现一次，很容易重复甚至引起恶意的回复攻击。因此，在一个TFO连接上的第一次请求就使用HTTP POST并不是一个明智的选择。更有问题的是，需要GET同样是有副作用的，但是浏览器并没有很好的方式来区分这些URL。

## TLS

TLS提供了另一种在TCP握手完成之后启动连接的方式。它看起来会是这样：

<div align="center"> <img src="https://infi-img.oss-cn-hangzhou.aliyuncs.com/img/TLS.jpg" style="display:block;width:50%;" alt="NAME" align=center /> </div>

在HTTP能够发送数据前需要两次完整的往复交互。在客户端达到服务端之前，`session tickets`允许你可以避免一次往复次数：

<div align="center"> <img src="https://infi-img.oss-cn-hangzhou.aliyuncs.com/img/TLS-session-tickets.jpg" style="display:block;width:50%;" alt="NAME" align=center /> </div>

很快，TLS将支持在客户端到达服务端之前提供“0往复次数”(zero round trip)的握手,换言之，HTTP可以在第一次往复中发送数据，避免额外的消耗。然而，和TFO一样，你需要能够确保第一次往复中发送的数据不会造成任何不好的影响。

## HTTP/next

TFO和TLS 1.3都是用来减少开启服务端连接的消耗。另一种方式尽量重用已开启的连接。

最后，这个讨论是如何使用HTTP/2的连接来使合并更加激烈(aggressively)，不仅仅是它是否能够帮助减少开启新连接的消耗，并且是能够使已存在的连接更加高效，就像TCP一样长寿命并且繁忙。

这些事情包括向客户端推送证书，以证明该连接能够跟他初始协定的一样被更多的源使用。

另外一个更彻底的改变正在被讨论：使用UDP替换TCP，比如QUIC(Goole定制的一种基于UDP的低延迟互联网传输协议)。